{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CI/CD for a Kubeflow pipeline on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "1. Learn how to create a custom Cloud Build builder to pilote Vertex AI Pipelines\n",
    "1. Learn how to write a Cloud Build config file to build and push all the artifacts for a KFP\n",
    "1. Learn how to setup a Cloud Build GitHub trigger a new run of the Kubeflow PIpeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will walk through authoring of a **Cloud Build** CI/CD workflow that automatically builds, deploys, and runs a Kubeflow pipeline on Vertex AI. You will also integrate your workflow with **GitHub** by setting up a trigger that starts the  workflow when a new tag is applied to the **GitHub** repo hosting the pipeline's code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "REGION = \"us-central1\"\n",
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}-kfp-artifact-store\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make sure that the artifact store exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-asl-00-61116abbd0d9-kfp-artifact-store/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the KFP CLI builder for Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In the cell below, write a docker file that\n",
    "* Uses `gcr.io/deeplearning-platform-release/base-cpu` as base image\n",
    "* Install the python packages `kfp` with version `2.4.0 `, `google-cloud-aiplatform` with version `1.43.0` and `fire`\n",
    "* Starts `/bin/bash` as entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing kfp-cli_vertex/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile kfp-cli_vertex/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U kfp==2.4.0 google-cloud-aiplatform==1.43.0 fire\n",
    "\n",
    "ENTRYPOINT [\"/bin/bash\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the image and push it to your project's **Artifact Registry**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-docker.pkg.dev/qwiklabs-asl-00-61116abbd0d9/asl-artifact-repo/kfp-cli-vertex:latest'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARTIFACT_REGISTRY_DIR = \"asl-artifact-repo\"\n",
    "KFP_CLI_IMAGE_NAME = \"kfp-cli-vertex\"\n",
    "KFP_CLI_IMAGE_URI = f\"us-docker.pkg.dev/{PROJECT_ID}/{ARTIFACT_REGISTRY_DIR}/{KFP_CLI_IMAGE_NAME}:latest\"\n",
    "KFP_CLI_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In the cell below, use `gcloud builds` to build the `kfp-cli-vertex` Docker image and push it to the project Artifact Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 1.1 KiB before compression.\n",
      "Uploading tarball of [kfp-cli_vertex] to [gs://qwiklabs-asl-00-61116abbd0d9_cloudbuild/source/1714048456.964252-94feb4b35d9d4e07bf5e59058f758ad7.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-asl-00-61116abbd0d9/locations/global/builds/d74af65c-e097-417e-9313-7bfa7c18d345].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/d74af65c-e097-417e-9313-7bfa7c18d345?project=865563922071 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"d74af65c-e097-417e-9313-7bfa7c18d345\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-asl-00-61116abbd0d9_cloudbuild/source/1714048456.964252-94feb4b35d9d4e07bf5e59058f758ad7.tgz#1714048457297883\n",
      "Copying gs://qwiklabs-asl-00-61116abbd0d9_cloudbuild/source/1714048456.964252-94feb4b35d9d4e07bf5e59058f758ad7.tgz#1714048457297883...\n",
      "/ [1 files][  923.0 B/  923.0 B]                                                \n",
      "Operation completed over 1 objects/923.0 B.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  3.584kB\n",
      "Step 1/3 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "3c67549075b6: Pulling fs layer\n",
      "f731e8575982: Pulling fs layer\n",
      "a17c744bff06: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "5f7fbc7b1e04: Pulling fs layer\n",
      "9d8a67ea3694: Pulling fs layer\n",
      "283df9fe4dc7: Pulling fs layer\n",
      "c0af149c563a: Pulling fs layer\n",
      "f1095fd7c21b: Pulling fs layer\n",
      "7ee940117aaf: Pulling fs layer\n",
      "a6d6a8d93183: Pulling fs layer\n",
      "df6e25f04f90: Pulling fs layer\n",
      "d2ec14755b77: Pulling fs layer\n",
      "8aab02f24d1d: Pulling fs layer\n",
      "6bec4bc7e842: Pulling fs layer\n",
      "d2388036fcad: Pulling fs layer\n",
      "5efb392ba437: Pulling fs layer\n",
      "6e1ec660ce76: Pulling fs layer\n",
      "f36f97d6b70e: Pulling fs layer\n",
      "9f2d931fc296: Pulling fs layer\n",
      "54ca31e009cf: Pulling fs layer\n",
      "58de6aaecf65: Pulling fs layer\n",
      "c288ead8a030: Pulling fs layer\n",
      "4e936ea22826: Pulling fs layer\n",
      "0287443cd6a7: Pulling fs layer\n",
      "84c21dea55a0: Pulling fs layer\n",
      "4f4fb700ef54: Waiting\n",
      "5f7fbc7b1e04: Waiting\n",
      "9d8a67ea3694: Waiting\n",
      "283df9fe4dc7: Waiting\n",
      "c0af149c563a: Waiting\n",
      "f1095fd7c21b: Waiting\n",
      "7ee940117aaf: Waiting\n",
      "a6d6a8d93183: Waiting\n",
      "df6e25f04f90: Waiting\n",
      "d2ec14755b77: Waiting\n",
      "8aab02f24d1d: Waiting\n",
      "6bec4bc7e842: Waiting\n",
      "d2388036fcad: Waiting\n",
      "5efb392ba437: Waiting\n",
      "6e1ec660ce76: Waiting\n",
      "f36f97d6b70e: Waiting\n",
      "9f2d931fc296: Waiting\n",
      "54ca31e009cf: Waiting\n",
      "58de6aaecf65: Waiting\n",
      "c288ead8a030: Waiting\n",
      "4e936ea22826: Waiting\n",
      "0287443cd6a7: Waiting\n",
      "84c21dea55a0: Waiting\n",
      "a17c744bff06: Verifying Checksum\n",
      "a17c744bff06: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "3c67549075b6: Verifying Checksum\n",
      "3c67549075b6: Download complete\n",
      "f731e8575982: Verifying Checksum\n",
      "f731e8575982: Download complete\n",
      "283df9fe4dc7: Download complete\n",
      "c0af149c563a: Verifying Checksum\n",
      "c0af149c563a: Download complete\n",
      "f1095fd7c21b: Verifying Checksum\n",
      "f1095fd7c21b: Download complete\n",
      "7ee940117aaf: Verifying Checksum\n",
      "7ee940117aaf: Download complete\n",
      "a6d6a8d93183: Verifying Checksum\n",
      "a6d6a8d93183: Download complete\n",
      "9d8a67ea3694: Verifying Checksum\n",
      "9d8a67ea3694: Download complete\n",
      "d2ec14755b77: Download complete\n",
      "8aab02f24d1d: Download complete\n",
      "6bec4bc7e842: Verifying Checksum\n",
      "6bec4bc7e842: Download complete\n",
      "d2388036fcad: Verifying Checksum\n",
      "d2388036fcad: Download complete\n",
      "5efb392ba437: Verifying Checksum\n",
      "5efb392ba437: Download complete\n",
      "6e1ec660ce76: Verifying Checksum\n",
      "6e1ec660ce76: Download complete\n",
      "f36f97d6b70e: Verifying Checksum\n",
      "f36f97d6b70e: Download complete\n",
      "5f7fbc7b1e04: Verifying Checksum\n",
      "5f7fbc7b1e04: Download complete\n",
      "9f2d931fc296: Verifying Checksum\n",
      "9f2d931fc296: Download complete\n",
      "54ca31e009cf: Verifying Checksum\n",
      "54ca31e009cf: Download complete\n",
      "58de6aaecf65: Verifying Checksum\n",
      "58de6aaecf65: Download complete\n",
      "3c67549075b6: Pull complete\n",
      "4e936ea22826: Verifying Checksum\n",
      "4e936ea22826: Download complete\n",
      "0287443cd6a7: Verifying Checksum\n",
      "0287443cd6a7: Download complete\n",
      "df6e25f04f90: Verifying Checksum\n",
      "df6e25f04f90: Download complete\n",
      "84c21dea55a0: Verifying Checksum\n",
      "84c21dea55a0: Download complete\n",
      "f731e8575982: Pull complete\n",
      "a17c744bff06: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "c288ead8a030: Verifying Checksum\n",
      "c288ead8a030: Download complete\n",
      "5f7fbc7b1e04: Pull complete\n",
      "9d8a67ea3694: Pull complete\n",
      "283df9fe4dc7: Pull complete\n",
      "c0af149c563a: Pull complete\n",
      "f1095fd7c21b: Pull complete\n",
      "7ee940117aaf: Pull complete\n",
      "a6d6a8d93183: Pull complete\n",
      "df6e25f04f90: Pull complete\n",
      "d2ec14755b77: Pull complete\n",
      "8aab02f24d1d: Pull complete\n",
      "6bec4bc7e842: Pull complete\n",
      "d2388036fcad: Pull complete\n",
      "5efb392ba437: Pull complete\n",
      "6e1ec660ce76: Pull complete\n",
      "f36f97d6b70e: Pull complete\n",
      "9f2d931fc296: Pull complete\n",
      "54ca31e009cf: Pull complete\n",
      "58de6aaecf65: Pull complete\n",
      "c288ead8a030: Pull complete\n",
      "4e936ea22826: Pull complete\n",
      "0287443cd6a7: Pull complete\n",
      "84c21dea55a0: Pull complete\n",
      "Digest: sha256:bde0532c3edeb8155a48569f30f1f5ff0067c51fd5a8cbde1fb67877b9c4c54e\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> fb33b4caff80\n",
      "Step 2/3 : RUN pip install -U kfp==2.4.0 google-cloud-aiplatform==1.43.0 fire\n",
      " ---> Running in e59d1f52d4f3\n",
      "Collecting kfp==2.4.0\n",
      "  Downloading kfp-2.4.0.tar.gz (392 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 392.3/392.3 kB 15.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting google-cloud-aiplatform==1.43.0\n",
      "  Downloading google_cloud_aiplatform-1.43.0-py2.py3-none-any.whl.metadata (27 kB)\n",
      "Collecting fire\n",
      "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.4/88.4 kB 9.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.4.0) (8.1.7)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.10/site-packages (from kfp==2.4.0) (0.16)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from kfp==2.4.0) (1.34.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from kfp==2.4.0) (2.29.0)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from kfp==2.4.0) (2.14.0)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.2.2 in /opt/conda/lib/python3.10/site-packages (from kfp==2.4.0) (0.2.2)\n",
      "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.4.0) (2.0.5)\n",
      "Requirement already satisfied: kubernetes<27,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.4.0) (26.1.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.4.0) (3.20.3)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from kfp==2.4.0) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.4.0) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from kfp==2.4.0) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.4.0) (1.26.18)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.43.0) (1.23.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.43.0) (24.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.43.0) (3.20.1)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.43.0) (1.12.3)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.43.0) (2.0.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire) (1.16.0)\n",
      "Collecting termcolor (from fire)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.4.0) (1.63.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.4.0) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.43.0) (1.62.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.43.0) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp==2.4.0) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp==2.4.0) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp==2.4.0) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.43.0) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.43.0) (2.7.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.43.0) (2.9.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.43.0) (0.13.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.4.0) (1.5.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.4.0) (2024.2.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp==2.4.0) (69.5.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp==2.4.0) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp==2.4.0) (2.0.0)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform==1.43.0) (1.25.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp==2.4.0) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.4.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.4.0) (3.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp==2.4.0) (3.2.2)\n",
      "Downloading google_cloud_aiplatform-1.43.0-py2.py3-none-any.whl (4.1 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.1/4.1 MB 79.8 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Building wheels for collected packages: kfp, fire\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-2.4.0-py3-none-any.whl size=542982 sha256=4b92b2cbf8614fc7ab0894d5f09a867add8584795e8f7f3d7231b07c42641562\n",
      "  Stored in directory: /root/.cache/pip/wheels/8f/68/87/2a7654c22807614c76ea1aefcdd6205a0f2035e64b958934ac\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=8f85f3d745e6ce1dfd238c3ad41bab48d14f269c921817a1c145a9e040d43d93\n",
      "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
      "Successfully built kfp fire\n",
      "Installing collected packages: termcolor, fire, kfp, google-cloud-aiplatform\n",
      "  Attempting uninstall: kfp\n",
      "    Found existing installation: kfp 2.5.0\n",
      "    Uninstalling kfp-2.5.0:\n",
      "      Successfully uninstalled kfp-2.5.0\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.48.0\n",
      "    Uninstalling google-cloud-aiplatform-1.48.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.48.0\n",
      "Successfully installed fire-0.6.0 google-cloud-aiplatform-1.43.0 kfp-2.4.0 termcolor-2.4.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container e59d1f52d4f3\n",
      " ---> 10cd5e8355d5\n",
      "Step 3/3 : ENTRYPOINT [\"/bin/bash\"]\n",
      " ---> Running in f83e798c4b68\n",
      "Removing intermediate container f83e798c4b68\n",
      " ---> bf50fe7831ea\n",
      "Successfully built bf50fe7831ea\n",
      "Successfully tagged us-docker.pkg.dev/qwiklabs-asl-00-61116abbd0d9/asl-artifact-repo/kfp-cli-vertex:latest\n",
      "PUSH\n",
      "Pushing us-docker.pkg.dev/qwiklabs-asl-00-61116abbd0d9/asl-artifact-repo/kfp-cli-vertex:latest\n",
      "The push refers to repository [us-docker.pkg.dev/qwiklabs-asl-00-61116abbd0d9/asl-artifact-repo/kfp-cli-vertex]\n",
      "39ade09683ac: Preparing\n",
      "c3139e9a1583: Preparing\n",
      "65451ec6b5ce: Preparing\n",
      "9114abfdadfa: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "88da1e2b4c08: Preparing\n",
      "060d68888ca0: Preparing\n",
      "f020d47bcdbd: Preparing\n",
      "9dfea1ce3033: Preparing\n",
      "133456cb43e7: Preparing\n",
      "ffdc3d9e3b1c: Preparing\n",
      "0cbfbe5b2dfd: Preparing\n",
      "fd72f2b2bbe3: Preparing\n",
      "025d33cc16cf: Preparing\n",
      "cd20b39c8599: Preparing\n",
      "4e229983a658: Preparing\n",
      "1c45225cf340: Preparing\n",
      "13b07c443c5e: Preparing\n",
      "43a55264fa95: Preparing\n",
      "93c176ca457b: Preparing\n",
      "f8d86ab58b06: Preparing\n",
      "adec7a0d17eb: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "a0b6dec6ae15: Preparing\n",
      "8d2004d502fb: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "900b5e3aa1c0: Preparing\n",
      "2ddbc9014f24: Preparing\n",
      "28da0445c449: Preparing\n",
      "88da1e2b4c08: Waiting\n",
      "060d68888ca0: Waiting\n",
      "f020d47bcdbd: Waiting\n",
      "9dfea1ce3033: Waiting\n",
      "133456cb43e7: Waiting\n",
      "ffdc3d9e3b1c: Waiting\n",
      "0cbfbe5b2dfd: Waiting\n",
      "fd72f2b2bbe3: Waiting\n",
      "025d33cc16cf: Waiting\n",
      "cd20b39c8599: Waiting\n",
      "4e229983a658: Waiting\n",
      "1c45225cf340: Waiting\n",
      "13b07c443c5e: Waiting\n",
      "43a55264fa95: Waiting\n",
      "93c176ca457b: Waiting\n",
      "f8d86ab58b06: Waiting\n",
      "adec7a0d17eb: Waiting\n",
      "a0b6dec6ae15: Waiting\n",
      "8d2004d502fb: Waiting\n",
      "900b5e3aa1c0: Waiting\n",
      "2ddbc9014f24: Waiting\n",
      "28da0445c449: Waiting\n",
      "5f70bf18a086: Layer already exists\n",
      "9114abfdadfa: Pushed\n",
      "65451ec6b5ce: Pushed\n",
      "060d68888ca0: Pushed\n",
      "f020d47bcdbd: Pushed\n",
      "39ade09683ac: Pushed\n",
      "9dfea1ce3033: Pushed\n",
      "133456cb43e7: Pushed\n",
      "ffdc3d9e3b1c: Pushed\n",
      "025d33cc16cf: Pushed\n",
      "fd72f2b2bbe3: Pushed\n",
      "c3139e9a1583: Pushed\n",
      "0cbfbe5b2dfd: Pushed\n",
      "cd20b39c8599: Pushed\n",
      "13b07c443c5e: Pushed\n",
      "4e229983a658: Pushed\n",
      "93c176ca457b: Pushed\n",
      "43a55264fa95: Pushed\n",
      "f8d86ab58b06: Pushed\n",
      "adec7a0d17eb: Pushed\n",
      "900b5e3aa1c0: Pushed\n",
      "2ddbc9014f24: Pushed\n",
      "28da0445c449: Layer already exists\n",
      "1c45225cf340: Pushed\n",
      "a0b6dec6ae15: Pushed\n",
      "8d2004d502fb: Pushed\n",
      "88da1e2b4c08: Pushed\n",
      "latest: digest: sha256:68969483c33346e61d7cad1656e7095c0e0828faae1536e11708ee9ecf347eb2 size: 6381\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                                                     STATUS\n",
      "d74af65c-e097-417e-9313-7bfa7c18d345  2024-04-25T12:34:17+00:00  7M50S     gs://qwiklabs-asl-00-61116abbd0d9_cloudbuild/source/1714048456.964252-94feb4b35d9d4e07bf5e59058f758ad7.tgz  us-docker.pkg.dev/qwiklabs-asl-00-61116abbd0d9/asl-artifact-repo/kfp-cli-vertex (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $KFP_CLI_IMAGE_URI kfp-cli_vertex   # COMPLETE THE COMMAND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the **Cloud Build** workflow.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "In the cell below, you'll complete the `cloudbuild_vertex.yaml` file describing the CI/CD workflow and prescribing how environment specific settings are abstracted using **Cloud Build** variables.\n",
    "\n",
    "The CI/CD workflow automates the steps you walked through manually during `lab-02_vertex`:\n",
    "1. Builds the trainer image\n",
    "1. Compiles the pipeline\n",
    "1. Uploads and run the pipeline to the Vertex AI Pipeline environment\n",
    "1. Pushes the trainer to your project's **Artifact Registry**\n",
    " \n",
    "\n",
    "The **Cloud Build** workflow configuration uses both standard and custom [Cloud Build builders](https://cloud.google.com/cloud-build/docs/cloud-builders). The custom builder encapsulates **KFP CLI**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cloudbuild_vertex.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile cloudbuild_vertex.yaml\n",
    "# Copyright 2021 Google LLC\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this\n",
    "# file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "    \n",
    "# Unless required by applicable law or agreed to in writing, software \n",
    "# distributed under the License is distributed on an \"AS IS\"\n",
    "# BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either \n",
    "# express or implied. See the License for the specific language governing \n",
    "# permissions and limitations under the License.\n",
    "\n",
    "steps:\n",
    "# Build the trainer image\n",
    "# TODO: Done\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  id: 'Build the training image'\n",
    "  args: ['build', '-t', 'us-docker.pkg.dev/$PROJECT_ID/asl-artifact-repo/trainer_image_covertype_vertex:latest','.']\n",
    "  dir:  $_PIPELINE_FOLDER/trainer_image_vertex\n",
    "\n",
    "# Push the trainer image, to make it available in the compile step\n",
    "# TODO: Done\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  id: 'Push the training image to the artifact repo' \n",
    "  args: ['push', 'us-docker.pkg.dev/$PROJECT_ID/asl-artifact-repo/trainer_image_covertype_vertex:latest']\n",
    "  dir:  $_PIPELINE_FOLDER/trainer_image_vertex\n",
    "\n",
    "\n",
    "# Compile the pipeline\n",
    "# TODO: Done\n",
    "- name: 'us-docker.pkg.dev/$PROJECT_ID/asl-artifact-repo/kfp-cli-vertex'\n",
    "  id: \"Compile kfp yaml for pipeline\"\n",
    "  args:\n",
    "  - '-c'\n",
    "  - |\n",
    "    kfp dsl compile --py pipeline.py --output covertype_kfp_pipeline.yaml \n",
    "  env:\n",
    "  - 'PIPELINE_ROOT=gs://$PROJECT_ID-kfp-artifact-store/pipeline'\n",
    "  - 'PROJECT_ID=$PROJECT_ID'\n",
    "  - 'REGION=$_REGION'\n",
    "  - 'SERVING_CONTAINER_IMAGE_URI=us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest'\n",
    "  - 'TRAINING_CONTAINER_IMAGE_URI=us-docker.pkg.dev/$PROJECT_ID/asl-artifact-repo/trainer_image_covertype_vertex:latest'\n",
    "  - 'TRAINING_FILE_PATH=gs://$PROJECT_ID-kfp-artifact-store/data/training/dataset.csv'\n",
    "  - 'VALIDATION_FILE_PATH=gs://$PROJECT_ID-kfp-artifact-store/data/validation/dataset.csv'\n",
    "  dir: $_PIPELINE_FOLDER/pipeline_vertex\n",
    "\n",
    "# Run the pipeline\n",
    "# TODO: Done\n",
    "- name: 'us-docker.pkg.dev/$PROJECT_ID/asl-artifact-repo/kfp-cli-vertex'\n",
    "  id: \"Run the pipeline\"\n",
    "  args:\n",
    "  - '-c'\n",
    "  - |\n",
    "    python $_PIPELINE_FOLDER/kfp-cli_vertex/run_pipeline.py \\\n",
    "    --project_id=$PROJECT_ID \\\n",
    "    --region=$_REGION \\\n",
    "    --template_path=$_PIPELINE_FOLDER/pipeline_vertex/covertype_kfp_pipeline.yaml \\\n",
    "    --display_name=\"covertype-kfp-pipeline\" \n",
    "   \n",
    "# Push the images to Artifact Registry\n",
    "# TODO: Done\n",
    "images: \n",
    "    - 'us-docker.pkg.dev/$PROJECT_ID/asl-artifact-repo/trainer_image_covertype_vertex:latest'\n",
    "\n",
    "\n",
    "# This is required since the pipeline run overflows the default timeout\n",
    "timeout: 10800s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually triggering CI/CD runs\n",
    "\n",
    "You can manually trigger **Cloud Build** runs using the [gcloud builds submit command]( https://cloud.google.com/sdk/gcloud/reference/builds/submit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_REGION=us-central1,_PIPELINE_FOLDER=.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBSTITUTIONS = f\"_REGION={REGION},_PIPELINE_FOLDER=.\"\n",
    "SUBSTITUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 13 file(s) totalling 107.3 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://qwiklabs-asl-00-61116abbd0d9_cloudbuild/source/1714055423.632605-80f85627ac99408daa382b6375b564b1.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-asl-00-61116abbd0d9/locations/global/builds/51d8fd39-d287-4b1d-8c6d-4ed157187098].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/51d8fd39-d287-4b1d-8c6d-4ed157187098?project=865563922071 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"51d8fd39-d287-4b1d-8c6d-4ed157187098\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-asl-00-61116abbd0d9_cloudbuild/source/1714055423.632605-80f85627ac99408daa382b6375b564b1.tgz#1714055423965022\n",
      "Copying gs://qwiklabs-asl-00-61116abbd0d9_cloudbuild/source/1714055423.632605-80f85627ac99408daa382b6375b564b1.tgz#1714055423965022...\n",
      "/ [1 files][ 21.3 KiB/ 21.3 KiB]                                                \n",
      "Operation completed over 1 objects/21.3 KiB.\n",
      "BUILD\n",
      "Starting Step #0 - \"Build the training image\"\n",
      "Step #0 - \"Build the training image\": Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #0 - \"Build the training image\": Sending build context to Docker daemon  6.144kB\n",
      "Step #0 - \"Build the training image\": Step 1/5 : FROM us-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-0\n",
      "Step #0 - \"Build the training image\": latest: Pulling from vertex-ai/training/sklearn-cpu.1-0\n",
      "Step #0 - \"Build the training image\": 3153aa388d02: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": 9824d45cbc78: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": a2c893dfdd76: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": 3fc58b7cea6b: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": 065e5d8a1c31: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": 3b608554e986: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": c3fb815e9074: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": 22edc53d84f1: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": fa13d373537f: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": 1ff377ed9aa4: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": 2854f30de59f: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": 5883a3d3350c: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": b9f16134ecfb: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": 739cdac78559: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": 482590cd5c44: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": fcd7dfefd59a: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": d7818bca27c1: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": af8e5ef74ae3: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": ee71ca03e59f: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": b531e85b45eb: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": aa0eae780ea1: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": caeb9ac3c484: Pulling fs layer\n",
      "Step #0 - \"Build the training image\": 3fc58b7cea6b: Waiting\n",
      "Step #0 - \"Build the training image\": 065e5d8a1c31: Waiting\n",
      "Step #0 - \"Build the training image\": 3b608554e986: Waiting\n",
      "Step #0 - \"Build the training image\": c3fb815e9074: Waiting\n",
      "Step #0 - \"Build the training image\": 22edc53d84f1: Waiting\n",
      "Step #0 - \"Build the training image\": fa13d373537f: Waiting\n",
      "Step #0 - \"Build the training image\": 1ff377ed9aa4: Waiting\n",
      "Step #0 - \"Build the training image\": 2854f30de59f: Waiting\n",
      "Step #0 - \"Build the training image\": 5883a3d3350c: Waiting\n",
      "Step #0 - \"Build the training image\": b9f16134ecfb: Waiting\n",
      "Step #0 - \"Build the training image\": 739cdac78559: Waiting\n",
      "Step #0 - \"Build the training image\": 482590cd5c44: Waiting\n",
      "Step #0 - \"Build the training image\": fcd7dfefd59a: Waiting\n",
      "Step #0 - \"Build the training image\": d7818bca27c1: Waiting\n",
      "Step #0 - \"Build the training image\": af8e5ef74ae3: Waiting\n",
      "Step #0 - \"Build the training image\": ee71ca03e59f: Waiting\n",
      "Step #0 - \"Build the training image\": b531e85b45eb: Waiting\n",
      "Step #0 - \"Build the training image\": aa0eae780ea1: Waiting\n",
      "Step #0 - \"Build the training image\": caeb9ac3c484: Waiting\n",
      "Step #0 - \"Build the training image\": 9824d45cbc78: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": 9824d45cbc78: Download complete\n",
      "Step #0 - \"Build the training image\": a2c893dfdd76: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": a2c893dfdd76: Download complete\n",
      "Step #0 - \"Build the training image\": 065e5d8a1c31: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": 065e5d8a1c31: Download complete\n",
      "Step #0 - \"Build the training image\": 3153aa388d02: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": 3153aa388d02: Download complete\n",
      "Step #0 - \"Build the training image\": c3fb815e9074: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": c3fb815e9074: Download complete\n",
      "Step #0 - \"Build the training image\": 3fc58b7cea6b: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": 3fc58b7cea6b: Download complete\n",
      "Step #0 - \"Build the training image\": 3b608554e986: Download complete\n",
      "Step #0 - \"Build the training image\": 1ff377ed9aa4: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": 1ff377ed9aa4: Download complete\n",
      "Step #0 - \"Build the training image\": 3153aa388d02: Pull complete\n",
      "Step #0 - \"Build the training image\": 2854f30de59f: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": 2854f30de59f: Download complete\n",
      "Step #0 - \"Build the training image\": 22edc53d84f1: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": 22edc53d84f1: Download complete\n",
      "Step #0 - \"Build the training image\": 9824d45cbc78: Pull complete\n",
      "Step #0 - \"Build the training image\": a2c893dfdd76: Pull complete\n",
      "Step #0 - \"Build the training image\": 5883a3d3350c: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": 5883a3d3350c: Download complete\n",
      "Step #0 - \"Build the training image\": 739cdac78559: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": 739cdac78559: Download complete\n",
      "Step #0 - \"Build the training image\": b9f16134ecfb: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": b9f16134ecfb: Download complete\n",
      "Step #0 - \"Build the training image\": fcd7dfefd59a: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": fcd7dfefd59a: Download complete\n",
      "Step #0 - \"Build the training image\": d7818bca27c1: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": d7818bca27c1: Download complete\n",
      "Step #0 - \"Build the training image\": af8e5ef74ae3: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": af8e5ef74ae3: Download complete\n",
      "Step #0 - \"Build the training image\": ee71ca03e59f: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": ee71ca03e59f: Download complete\n",
      "Step #0 - \"Build the training image\": b531e85b45eb: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": b531e85b45eb: Download complete\n",
      "Step #0 - \"Build the training image\": aa0eae780ea1: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": aa0eae780ea1: Download complete\n",
      "Step #0 - \"Build the training image\": caeb9ac3c484: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": caeb9ac3c484: Download complete\n",
      "Step #0 - \"Build the training image\": 482590cd5c44: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": 482590cd5c44: Download complete\n",
      "Step #0 - \"Build the training image\": fa13d373537f: Verifying Checksum\n",
      "Step #0 - \"Build the training image\": fa13d373537f: Download complete\n",
      "Step #0 - \"Build the training image\": 3fc58b7cea6b: Pull complete\n",
      "Step #0 - \"Build the training image\": 065e5d8a1c31: Pull complete\n",
      "Step #0 - \"Build the training image\": 3b608554e986: Pull complete\n",
      "Step #0 - \"Build the training image\": c3fb815e9074: Pull complete\n",
      "Step #0 - \"Build the training image\": 22edc53d84f1: Pull complete\n",
      "Step #0 - \"Build the training image\": fa13d373537f: Pull complete\n",
      "Step #0 - \"Build the training image\": 1ff377ed9aa4: Pull complete\n",
      "Step #0 - \"Build the training image\": 2854f30de59f: Pull complete\n",
      "Step #0 - \"Build the training image\": 5883a3d3350c: Pull complete\n",
      "Step #0 - \"Build the training image\": b9f16134ecfb: Pull complete\n",
      "Step #0 - \"Build the training image\": 739cdac78559: Pull complete\n",
      "Step #0 - \"Build the training image\": 482590cd5c44: Pull complete\n",
      "Step #0 - \"Build the training image\": fcd7dfefd59a: Pull complete\n",
      "Step #0 - \"Build the training image\": d7818bca27c1: Pull complete\n",
      "Step #0 - \"Build the training image\": af8e5ef74ae3: Pull complete\n",
      "Step #0 - \"Build the training image\": ee71ca03e59f: Pull complete\n",
      "Step #0 - \"Build the training image\": b531e85b45eb: Pull complete\n",
      "Step #0 - \"Build the training image\": aa0eae780ea1: Pull complete\n",
      "Step #0 - \"Build the training image\": caeb9ac3c484: Pull complete\n",
      "Step #0 - \"Build the training image\": Digest: sha256:c1bc11aed8635650a6a1a1aebe7cabae3b2019c6e08f7aabf3d395b73442ba4f\n",
      "Step #0 - \"Build the training image\": Status: Downloaded newer image for us-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-0:latest\n",
      "Step #0 - \"Build the training image\":  ---> 57d74f998f2a\n",
      "Step #0 - \"Build the training image\": Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==1.2.2\n",
      "Step #0 - \"Build the training image\":  ---> Running in eee2b378253d\n",
      "Step #0 - \"Build the training image\": Collecting fire\n",
      "Step #0 - \"Build the training image\":   Downloading fire-0.6.0.tar.gz (88 kB)\n",
      "Step #0 - \"Build the training image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.4/88.4 kB 4.8 MB/s eta 0:00:00\n",
      "Step #0 - \"Build the training image\":   Preparing metadata (setup.py): started\n",
      "Step #0 - \"Build the training image\":   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0 - \"Build the training image\": Requirement already satisfied: cloudml-hypertune in /opt/conda/lib/python3.10/site-packages (0.1.0.dev6)\n",
      "Step #0 - \"Build the training image\": Collecting scikit-learn==1.2.2\n",
      "Step #0 - \"Build the training image\":   Obtaining dependency information for scikit-learn==1.2.2 from https://files.pythonhosted.org/packages/fa/1e/36d7609e84b50d4a2e5bc43cd5013d9ea885799e5813a1e9cf5bb1afd3f4/scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "Step #0 - \"Build the training image\":   Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Step #0 - \"Build the training image\": Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.25.1)\n",
      "Step #0 - \"Build the training image\": Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.11.1)\n",
      "Step #0 - \"Build the training image\": Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.3.1)\n",
      "Step #0 - \"Build the training image\": Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (3.2.0)\n",
      "Step #0 - \"Build the training image\": Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire) (1.16.0)\n",
      "Step #0 - \"Build the training image\": Requirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire) (2.3.0)\n",
      "Step #0 - \"Build the training image\": Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "Step #0 - \"Build the training image\":    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.6/9.6 MB 81.3 MB/s eta 0:00:00\n",
      "Step #0 - \"Build the training image\": Building wheels for collected packages: fire\n",
      "Step #0 - \"Build the training image\":   Building wheel for fire (setup.py): started\n",
      "Step #0 - \"Build the training image\":   Building wheel for fire (setup.py): finished with status 'done'\n",
      "Step #0 - \"Build the training image\":   Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117031 sha256=f0f634233e5547164fad439aa68cb30fee70012318a59ed8be0ffbf0be565f7d\n",
      "Step #0 - \"Build the training image\":   Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
      "Step #0 - \"Build the training image\": Successfully built fire\n",
      "Step #0 - \"Build the training image\": Installing collected packages: fire, scikit-learn\n",
      "Step #0 - \"Build the training image\":   Attempting uninstall: scikit-learn\n",
      "Step #0 - \"Build the training image\":     Found existing installation: scikit-learn 1.0.2\n",
      "Step #0 - \"Build the training image\":     Uninstalling scikit-learn-1.0.2:\n",
      "Step #0 - \"Build the training image\":       Successfully uninstalled scikit-learn-1.0.2\n",
      "Step #0 - \"Build the training image\": Successfully installed fire-0.6.0 scikit-learn-1.2.2\n",
      "Step #0 - \"Build the training image\": \u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #0 - \"Build the training image\": \u001b[0mRemoving intermediate container eee2b378253d\n",
      "Step #0 - \"Build the training image\":  ---> ad726f34357c\n",
      "Step #0 - \"Build the training image\": Step 3/5 : WORKDIR /app\n",
      "Step #0 - \"Build the training image\":  ---> Running in d24126f164d6\n",
      "Step #0 - \"Build the training image\": Removing intermediate container d24126f164d6\n",
      "Step #0 - \"Build the training image\":  ---> deb41fa6d9df\n",
      "Step #0 - \"Build the training image\": Step 4/5 : COPY train.py .\n",
      "Step #0 - \"Build the training image\":  ---> 0f5efb5f6c08\n",
      "Step #0 - \"Build the training image\": Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      "Step #0 - \"Build the training image\":  ---> Running in 68bb5393f30d\n",
      "Step #0 - \"Build the training image\": Removing intermediate container 68bb5393f30d\n",
      "Step #0 - \"Build the training image\":  ---> 492a22e7c591\n",
      "Step #0 - \"Build the training image\": Successfully built 492a22e7c591\n",
      "Step #0 - \"Build the training image\": Successfully tagged us-docker.pkg.dev/qwiklabs-asl-00-61116abbd0d9/asl-artifact-repo/trainer_image_covertype_vertex:latest\n",
      "Finished Step #0 - \"Build the training image\"\n",
      "Starting Step #1 - \"Push the training image to the artifact repo\"\n",
      "Step #1 - \"Push the training image to the artifact repo\": Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #1 - \"Push the training image to the artifact repo\": The push refers to repository [us-docker.pkg.dev/qwiklabs-asl-00-61116abbd0d9/asl-artifact-repo/trainer_image_covertype_vertex]\n",
      "Step #1 - \"Push the training image to the artifact repo\": 9a28beddc49d: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": cd852c4f88eb: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": a5e8a4081e47: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": e42695c7b436: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": e42695c7b436: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": 7e34967c8575: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": 685157cbfd1c: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": b3f395f9c6db: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": 1335b11763b0: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": 1335b11763b0: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": ff1b3a6f8e32: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": a4f92d8c3713: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": 91315c4af0d6: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": f52f425121a7: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": 76305005ccce: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": 9c8c4140a22f: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": cb24f57d328e: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": cb24f57d328e: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": e9ac191ee4d5: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": e366492c2e8f: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": b3b47f6e19c9: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": 9fc6ad9fa2bc: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": 926e17c0ab1b: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": 299edaab5a5d: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": 662e1ac55b04: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": a1741b153e96: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": a8bc7d9be9d9: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": 59c56aee1fb4: Preparing\n",
      "Step #1 - \"Push the training image to the artifact repo\": 685157cbfd1c: Waiting\n",
      "Step #1 - \"Push the training image to the artifact repo\": b3f395f9c6db: Waiting\n",
      "Step #1 - \"Push the training image to the artifact repo\": 1335b11763b0: Waiting\n",
      "Step #1 - \"Push the training image to the artifact repo\": ff1b3a6f8e32: Waiting\n",
      "Step #1 - \"Push the training image to the artifact repo\": a4f92d8c3713: Waiting\n",
      "Step #1 - \"Push the training image to the artifact repo\": 91315c4af0d6: Waiting\n",
      "Step #1 - \"Push the training image to the artifact repo\": f52f425121a7: Waiting\n",
      "Step #1 - \"Push the training image to the artifact repo\": 76305005ccce: Waiting\n",
      "Step #1 - \"Push the training image to the artifact repo\": 9c8c4140a22f: Waiting\n",
      "Step #1 - \"Push the training image to the artifact repo\": cb24f57d328e: Waiting\n",
      "Step #1 - \"Push the training image to the artifact repo\": e9ac191ee4d5: Waiting\n",
      "Step #1 - \"Push the training image to the artifact repo\": e366492c2e8f: Waiting\n",
      "Step #1 - \"Push the training image to the artifact repo\": b3b47f6e19c9: Waiting\n",
      "Step #1 - \"Push the training image to the artifact repo\": 9fc6ad9fa2bc: Waiting\n",
      "Step #1 - \"Push the training image to the artifact repo\": 926e17c0ab1b: Waiting\n",
      "Step #1 - \"Push the training image to the artifact repo\": 299edaab5a5d: Waiting\n",
      "Step #1 - \"Push the training image to the artifact repo\": 662e1ac55b04: Waiting\n",
      "Step #1 - \"Push the training image to the artifact repo\": a1741b153e96: Waiting\n",
      "Step #1 - \"Push the training image to the artifact repo\": a8bc7d9be9d9: Waiting\n",
      "Step #1 - \"Push the training image to the artifact repo\": 59c56aee1fb4: Waiting\n",
      "Step #1 - \"Push the training image to the artifact repo\": 7e34967c8575: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": e42695c7b436: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": 685157cbfd1c: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": b3f395f9c6db: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": 1335b11763b0: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": ff1b3a6f8e32: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": 9a28beddc49d: Pushed\n",
      "Step #1 - \"Push the training image to the artifact repo\": a4f92d8c3713: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": cd852c4f88eb: Pushed\n",
      "Step #1 - \"Push the training image to the artifact repo\": f52f425121a7: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": 76305005ccce: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": 9c8c4140a22f: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": 91315c4af0d6: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": cb24f57d328e: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": e9ac191ee4d5: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": b3b47f6e19c9: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": 9fc6ad9fa2bc: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": e366492c2e8f: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": a1741b153e96: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": 926e17c0ab1b: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": 662e1ac55b04: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": 299edaab5a5d: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": a8bc7d9be9d9: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": 59c56aee1fb4: Layer already exists\n",
      "Step #1 - \"Push the training image to the artifact repo\": a5e8a4081e47: Pushed\n",
      "Step #1 - \"Push the training image to the artifact repo\": latest: digest: sha256:c212de519f2e21c1fec1db6d2cbdd81639eaeaa3311e5c61d03aa72375955966 size: 6171\n",
      "Finished Step #1 - \"Push the training image to the artifact repo\"\n",
      "Starting Step #2 - \"Compile kfp yaml for pipeline\"\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": Pulling image: us-docker.pkg.dev/qwiklabs-asl-00-61116abbd0d9/asl-artifact-repo/kfp-cli-vertex\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": Using default tag: latest\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": latest: Pulling from qwiklabs-asl-00-61116abbd0d9/asl-artifact-repo/kfp-cli-vertex\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 3c67549075b6: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": f731e8575982: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": a17c744bff06: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 4f4fb700ef54: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 5f7fbc7b1e04: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 9d8a67ea3694: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 283df9fe4dc7: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": c0af149c563a: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": f1095fd7c21b: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 7ee940117aaf: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": a6d6a8d93183: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": df6e25f04f90: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": d2ec14755b77: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 8aab02f24d1d: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 6bec4bc7e842: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": d2388036fcad: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 5efb392ba437: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 6e1ec660ce76: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": f36f97d6b70e: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 9f2d931fc296: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 54ca31e009cf: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 58de6aaecf65: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 4f4fb700ef54: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 5f7fbc7b1e04: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": c288ead8a030: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 4e936ea22826: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 9d8a67ea3694: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 283df9fe4dc7: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": c0af149c563a: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": f1095fd7c21b: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 7ee940117aaf: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 0287443cd6a7: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 84c21dea55a0: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": d11b0393b25b: Pulling fs layer\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": a6d6a8d93183: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": df6e25f04f90: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": d2ec14755b77: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 8aab02f24d1d: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 6bec4bc7e842: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": d2388036fcad: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 5efb392ba437: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 6e1ec660ce76: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": f36f97d6b70e: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 9f2d931fc296: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 54ca31e009cf: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 58de6aaecf65: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": c288ead8a030: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 4e936ea22826: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 0287443cd6a7: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 84c21dea55a0: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": d11b0393b25b: Waiting\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": a17c744bff06: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": a17c744bff06: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 4f4fb700ef54: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 4f4fb700ef54: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 3c67549075b6: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 3c67549075b6: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": f731e8575982: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": f731e8575982: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 283df9fe4dc7: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 283df9fe4dc7: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": c0af149c563a: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": c0af149c563a: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": f1095fd7c21b: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 9d8a67ea3694: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 9d8a67ea3694: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": a6d6a8d93183: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": a6d6a8d93183: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 7ee940117aaf: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 7ee940117aaf: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 3c67549075b6: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": d2ec14755b77: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 8aab02f24d1d: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 8aab02f24d1d: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 6bec4bc7e842: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 6bec4bc7e842: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": d2388036fcad: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": d2388036fcad: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 5f7fbc7b1e04: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 5f7fbc7b1e04: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 5efb392ba437: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 5efb392ba437: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 6e1ec660ce76: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 6e1ec660ce76: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": f36f97d6b70e: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": f36f97d6b70e: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 9f2d931fc296: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 9f2d931fc296: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 54ca31e009cf: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 54ca31e009cf: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 58de6aaecf65: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": df6e25f04f90: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": df6e25f04f90: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 4e936ea22826: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 4e936ea22826: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 0287443cd6a7: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 0287443cd6a7: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": f731e8575982: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": d11b0393b25b: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": d11b0393b25b: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": a17c744bff06: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 4f4fb700ef54: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 84c21dea55a0: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 84c21dea55a0: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": c288ead8a030: Verifying Checksum\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": c288ead8a030: Download complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 5f7fbc7b1e04: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 9d8a67ea3694: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 283df9fe4dc7: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": c0af149c563a: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": f1095fd7c21b: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 7ee940117aaf: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": a6d6a8d93183: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": df6e25f04f90: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": d2ec14755b77: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 8aab02f24d1d: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 6bec4bc7e842: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": d2388036fcad: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 5efb392ba437: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 6e1ec660ce76: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": f36f97d6b70e: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 9f2d931fc296: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 54ca31e009cf: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 58de6aaecf65: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": c288ead8a030: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 4e936ea22826: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 0287443cd6a7: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": 84c21dea55a0: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": d11b0393b25b: Pull complete\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": Digest: sha256:68969483c33346e61d7cad1656e7095c0e0828faae1536e11708ee9ecf347eb2\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": Status: Downloaded newer image for us-docker.pkg.dev/qwiklabs-asl-00-61116abbd0d9/asl-artifact-repo/kfp-cli-vertex:latest\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": us-docker.pkg.dev/qwiklabs-asl-00-61116abbd0d9/asl-artifact-repo/kfp-cli-vertex:latest\n",
      "Step #2 - \"Compile kfp yaml for pipeline\": /workspace/pipeline_vertex/covertype_kfp_pipeline.yaml\n",
      "Finished Step #2 - \"Compile kfp yaml for pipeline\"\n",
      "Starting Step #3 - \"Run the pipeline\"\n",
      "Step #3 - \"Run the pipeline\": Already have image (with digest): us-docker.pkg.dev/qwiklabs-asl-00-61116abbd0d9/asl-artifact-repo/kfp-cli-vertex\n",
      "Step #3 - \"Run the pipeline\": Creating PipelineJob\n",
      "Step #3 - \"Run the pipeline\": PipelineJob created. Resource name: projects/865563922071/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20240425143533\n",
      "Step #3 - \"Run the pipeline\": To use this PipelineJob in another session:\n",
      "Step #3 - \"Run the pipeline\": pipeline_job = aiplatform.PipelineJob.get('projects/865563922071/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20240425143533')\n",
      "Step #3 - \"Run the pipeline\": View Pipeline Job:\n",
      "Step #3 - \"Run the pipeline\": https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/covertype-kfp-pipeline-20240425143533?project=865563922071\n",
      "Step #3 - \"Run the pipeline\": PipelineJob projects/865563922071/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20240425143533 current state:\n",
      "Step #3 - \"Run the pipeline\": PipelineState.PIPELINE_STATE_RUNNING\n",
      "Step #3 - \"Run the pipeline\": PipelineJob projects/865563922071/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20240425143533 current state:\n",
      "Step #3 - \"Run the pipeline\": PipelineState.PIPELINE_STATE_RUNNING\n",
      "Step #3 - \"Run the pipeline\": PipelineJob projects/865563922071/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20240425143533 current state:\n",
      "Step #3 - \"Run the pipeline\": PipelineState.PIPELINE_STATE_RUNNING\n",
      "Step #3 - \"Run the pipeline\": PipelineJob projects/865563922071/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20240425143533 current state:\n",
      "Step #3 - \"Run the pipeline\": PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit . --config cloudbuild_vertex.yaml --substitutions {SUBSTITUTIONS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you experience issues with CloudBuild being able to access Vertex AI, you may need to run the following commands in **CloudShell**:\n",
    "\n",
    "```\n",
    "PROJECT_ID=$(gcloud config get-value project)\n",
    "PROJECT_NUMBER=$(gcloud projects list --filter=\"name=$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\")\n",
    "\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "  --member serviceAccount:$PROJECT_NUMBER@cloudbuild.gserviceaccount.com \\\n",
    "  --role roles/editor\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member serviceAccount:$PROJECT_NUMBER-compute@developer.gserviceaccount.com \\\n",
    "    --role roles/storage.objectAdmin\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up GitHub integration\n",
    "\n",
    "## Exercise\n",
    "\n",
    "In this exercise you integrate your CI/CD workflow with **GitHub**, using [Cloud Build GitHub App](https://github.com/marketplace/google-cloud-build). \n",
    "You will set up a trigger that starts the CI/CD workflow when a new tag is applied to the **GitHub** repo managing the  pipeline source code. You will use a fork of this repo as your source GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a fork of this repo\n",
    "[Follow the GitHub documentation](https://help.github.com/en/github/getting-started-with-github/fork-a-repo) to fork [this repo](https://github.com/GoogleCloudPlatform/asl-ml-immersion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Reflect yaml file change and create a commit\n",
    "\n",
    "Go to your fork of this repo page, and open `asl-ml-immersion/notebooks/kubeflow_pipelines/cicd/labs/cloudbuild_vertex.yaml` file.\n",
    "\n",
    "Click Edit button and copy your updated yaml file directly to the page.\n",
    "![image](https://user-images.githubusercontent.com/6895245/158727133-e5d77f0c-354c-4b2b-a710-8209ee67571f.png)\n",
    "\n",
    "Click 'Commit changes' button and create a new commit. \n",
    "![image](https://user-images.githubusercontent.com/6895245/158727565-13b4981a-8bce-401b-8f1a-d09a33a163a8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create a **Cloud Build** trigger\n",
    "\n",
    "Connect the fork you created in the previous step to your Google Cloud project and create a trigger following the steps in the [Creating GitHub app trigger](https://cloud.google.com/cloud-build/docs/create-github-app-triggers) article. Use the following values on the **Edit trigger** form:\n",
    "\n",
    "|Field|Value|\n",
    "|-----|-----|\n",
    "|Name|[YOUR TRIGGER NAME]|\n",
    "|Description|[YOUR TRIGGER DESCRIPTION]|\n",
    "|Event| Tag|\n",
    "|Source| [YOUR FORK]|\n",
    "|Tag (regex)|.\\*|\n",
    "|Build Configuration|Cloud Build configuration file (yaml or json)|\n",
    "|Cloud Build configuration file location| ./notebooks/kubeflow_pipelines/cicd/labs/cloudbuild_vertex.yaml|\n",
    "\n",
    "\n",
    "Use the following values for the substitution variables:\n",
    "\n",
    "|Variable|Value|\n",
    "|--------|-----|\n",
    "|_REGION|us-central1|\n",
    "|_PIPELINE_FOLDER|notebooks/kubeflow_pipelines/cicd/labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Trigger the build\n",
    "\n",
    "To start an automated build [create a new release of the repo in GitHub](https://help.github.com/en/github/administering-a-repository/creating-releases). Alternatively, you can start the build by applying a tag using `git`. \n",
    "```\n",
    "git tag [TAG NAME]\n",
    "git push origin --tags\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the command above, a build should have been automatically triggered, which you should able to inspect [here](https://console.cloud.google.com/cloud-build/builds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-12.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-12:m119"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
